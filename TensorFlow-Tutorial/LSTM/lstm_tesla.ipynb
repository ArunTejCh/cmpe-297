{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "%matplotlib inline\n",
    "\n",
    "tesla_stocks = pd.read_csv('tesla_stocks.csv')\n",
    "tesla_stocks.head()\n",
    "\n",
    "data_to_use = tesla_stocks['Close'].values\n",
    "print('Total number of days in the dataset: {}'.format(len(data_to_use)))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaled_dataset = scaler.fit_transform(data_to_use.reshape(-1, 1))\n",
    "\n",
    "plt.figure(figsize=(12,7), frameon=False, facecolor='brown', edgecolor='blue')\n",
    "plt.title('Scaled TESLA stocks from August 2014 to August 2017')\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Scaled value of stocks')\n",
    "plt.plot(scaled_dataset, label='Stocks data')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "def window_data(data, window_size):\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    i = 0\n",
    "    while (i + window_size) <= len(data) - 1:\n",
    "        X.append(data[i:i+window_size])\n",
    "        y.append(data[i+window_size])\n",
    "        \n",
    "        i += 1\n",
    "    assert len(X) ==  len(y)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "X, y = window_data(scaled_dataset, 7)\n",
    "\n",
    "X_train  = np.array(X[:700])\n",
    "y_train = np.array(y[:700])\n",
    "\n",
    "X_test = np.array(X[700:])\n",
    "y_test = np.array(y[700:])\n",
    "\n",
    "print(\"X_train size: {}\".format(X_train.shape))\n",
    "print(\"y_train size: {}\".format(y_train.shape))\n",
    "print(\"X_test size: {}\".format(X_test.shape))\n",
    "print(\"y_test size: {}\".format(y_test.shape))\n",
    "\n",
    "\n",
    "epochs = 200\n",
    "batch_size = 7\n",
    "\n",
    "def LSTM_cell(hidden_layer_size, batch_size,number_of_layers, dropout=True, dropout_rate=0.8):\n",
    "    \n",
    "    layer = tf.contrib.rnn.BasicLSTMCell(hidden_layer_size)\n",
    "    \n",
    "    if dropout:\n",
    "        layer = tf.contrib.rnn.DropoutWrapper(layer, output_keep_prob=dropout_rate)\n",
    "        \n",
    "    cell = tf.contrib.rnn.MultiRNNCell([layer]*number_of_layers)\n",
    "    \n",
    "    init_state = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    return cell, init_state\n",
    "\n",
    "def output_layer(lstm_output, in_size, out_size):\n",
    "    \n",
    "    x = lstm_output[:, -1, :]\n",
    "    print(x)\n",
    "    weights = tf.Variable(tf.truncated_normal([in_size, out_size], stddev=0.05), name='output_layer_weights')\n",
    "    bias = tf.Variable(tf.zeros([out_size]), name='output_layer_bias')\n",
    "    \n",
    "    output = tf.matmul(x, weights) + bias\n",
    "    return output\n",
    "\n",
    "def opt_loss(logits, targets, learning_rate, grad_clip_margin):\n",
    "    \n",
    "    losses = []\n",
    "    for i in range(targets.get_shape()[0]):\n",
    "        losses.append([(tf.pow(logits[i] - targets[i], 2))])\n",
    "        \n",
    "    loss = tf.reduce_sum(losses)/(2*batch_size)\n",
    "    \n",
    "    #Cliping the gradient loss\n",
    "    gradients = tf.gradients(loss, tf.trainable_variables())\n",
    "    clipper_, _ = tf.clip_by_global_norm(gradients, grad_clip_margin)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    train_optimizer = optimizer.apply_gradients(zip(gradients, tf.trainable_variables()))\n",
    "    return loss, train_optimizer\n",
    "\n",
    "class StockPredictionRNN(object):\n",
    "    \n",
    "    def __init__(self, learning_rate=0.001, batch_size=7, hidden_layer_size=512, number_of_layers=1, \n",
    "                 dropout=True, dropout_rate=0.8, number_of_classes=1, gradient_clip_margin=4, window_size=7):\n",
    "    \n",
    "        self.inputs = tf.placeholder(tf.float32, [batch_size, window_size, 1], name='input_data')\n",
    "        self.targets = tf.placeholder(tf.float32, [batch_size, 1], name='targets')\n",
    "\n",
    "        cell, init_state = LSTM_cell(hidden_layer_size, batch_size, number_of_layers, dropout, dropout_rate)\n",
    "\n",
    "        outputs, states = tf.nn.dynamic_rnn(cell, self.inputs, initial_state=init_state)\n",
    "\n",
    "        self.logits = output_layer(outputs, hidden_layer_size, number_of_classes)\n",
    "\n",
    "        self.loss, self.opt = opt_loss(self.logits, self.targets, learning_rate, gradient_clip_margin)\n",
    "        \n",
    "tf.reset_default_graph()\n",
    "model = StockPredictionRNN()\n",
    "\n",
    "session =  tf.Session()\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "for i in range(epochs):\n",
    "    traind_scores = []\n",
    "    ii = 0\n",
    "    epoch_loss = []\n",
    "    while(ii + batch_size) <= len(X_train):\n",
    "        X_batch = X_train[ii:ii+batch_size]\n",
    "        y_batch = y_train[ii:ii+batch_size]\n",
    "        \n",
    "        o, c, _ = session.run([model.logits, model.loss, model.opt], feed_dict={model.inputs:X_batch, model.targets:y_batch})\n",
    "        \n",
    "        epoch_loss.append(c)\n",
    "        traind_scores.append(o)\n",
    "        ii += batch_size\n",
    "    if (i % 30) == 0:\n",
    "        print('Epoch {}/{}'.format(i, epochs), ' Current loss: {}'.format(np.mean(epoch_loss)))\n",
    "\n",
    "sup =[]\n",
    "for i in range(len(traind_scores)):\n",
    "    for j in range(len(traind_scores[i])):\n",
    "        sup.append(traind_scores[i][j])\n",
    "        \n",
    "tests = []\n",
    "i = 0\n",
    "while i+batch_size <= len(X_test):\n",
    "    \n",
    "    o = session.run([model.logits], feed_dict={model.inputs:X_test[i:i+batch_size]})\n",
    "    i += batch_size\n",
    "    tests.append(o)\n",
    "\n",
    "tests_new = []\n",
    "for i in range(len(tests)):\n",
    "    for j in range(len(tests[i][0])):\n",
    "        tests_new.append(tests[i][0][j])\n",
    "        \n",
    "test_results = []\n",
    "for i in range(749):\n",
    "    if i >= 701:\n",
    "        test_results.append(tests_new[i-701])\n",
    "    else:\n",
    "        test_results.append(None)\n",
    "        \n",
    "\n",
    "plt.figure(figsize=(16, 7))\n",
    "plt.plot(scaled_dataset, label='Original data')\n",
    "plt.plot(sup, label='Training data')\n",
    "plt.plot(test_results, label='Testing data')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "session.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
